{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = ['twitter-2013train.txt','twitter-2015train.txt','twitter-2016train.txt']\n",
    "files = ['twitter-2013train.txt','twitter-2015train.txt','twitter-2016train.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0, df1, df2 = [pd.read_csv(name, delimiter = '\\t', header = None) for name in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df0, df1, df2], ignore_index=True) #concatinating the tweets data in 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['serial', 'opinion', 'tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opinion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2374</td>\n",
       "      <td>2374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>6840</td>\n",
       "      <td>6840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>6827</td>\n",
       "      <td>6827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          serial  tweet_text\n",
       "opinion                     \n",
       "negative    2374        2374\n",
       "neutral     6840        6840\n",
       "positive    6827        6827"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(by = 'opinion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16041, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>opinion</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               serial   opinion  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...  \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...  \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clean up & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: casefold\n",
    "\n",
    "import nltk\n",
    "\n",
    "lowerTweets =[]\n",
    "for tweet in data['tweet_text']:\n",
    "    lowerTweets.append(tweet.casefold())\n",
    "#lowerTweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Baz-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: remove stopwords applying on all tweets \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "filtered_tweets =[]\n",
    "for doc in lowerTweets:\n",
    "    curr = \"\"\n",
    "    for word in  re.split(\"\\W+\",doc):\n",
    "        if word not in stops: \n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    filtered_tweets.append(curr)\n",
    "#filtered_tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove punctuation and digits from tweets and replace it by space\n",
    "#### NOTE: Through different combinations, it is observed that accuracy is decreased after removing punctuation.\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    output = []\n",
    "    for tweet in input_text:\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        output.append(tweet.translate(trantab))\n",
    "    return output\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    out_list = []\n",
    "    for j in input_text:\n",
    "        out_list.append(re.sub('\\d+', '', j))\n",
    "    return out_list\n",
    "\n",
    "punctuation_removed_tweets = remove_punctuation(filtered_tweets)\n",
    "punctuation_removed_tweets[0:5]\n",
    "\n",
    "# Here we will skip removing the punctuation,\n",
    "# and will use the \"remove_digits\" function with \"filtered_tweets\" (output of stopwords)\n",
    "digits_removed_tweets = remove_digits(filtered_tweets)\n",
    "#digits_removed_tweets[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform trimming to remove extra whitespaces:\n",
    "\n",
    "spaces_removed_tweets = []\n",
    "for j in digits_removed_tweets:\n",
    "    spaces_removed_tweets.append(\" \".join(j.split()))\n",
    "\n",
    "#spaces_removed_tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: stemwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stemDocs(f_docs):\n",
    "    stemmed_docs =[]\n",
    "    for doc in  f_docs:\n",
    "        curr = \"\"\n",
    "        for word in doc.split():  \n",
    "            curr = curr + PorterStemmer().stem(word) +\" \"\n",
    "        curr = curr.strip()\n",
    "        stemmed_docs.append(curr)\n",
    "    return  stemmed_docs\n",
    "    \n",
    "stemmed_tweets = stemDocs(spaces_removed_tweets)\n",
    "#stemmed_tweets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After multiple trials with different combinations, the highest accuracy (64.99%) is reached through the below steps:\n",
    "\n",
    "### 1- Casefolding\n",
    "### 2- Remove stopwords\n",
    "### 3- Remove digits\n",
    "### 4- Trimming (remove whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>opinion</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>non_stemmed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "      <td>ga hous hit um go chapel hill sat</td>\n",
       "      <td>gas house hit um going chapel hill sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "      <td>theo walcott still shit uc watch rafa johnni d...</td>\n",
       "      <td>theo walcott still shit uc watch rafa johnny d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "      <td>um gsp fan uc hate nick diaz ut wait februari</td>\n",
       "      <td>um gsp fan uc hate nick diaz ut wait february</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "      <td>iranian gener say israel us iron dome ut deal ...</td>\n",
       "      <td>iranian general says israel us iron dome ut de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "      <td>tehran uc mon amour obama tri establish tie mu...</td>\n",
       "      <td>tehran uc mon amour obama tried establish ties...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               serial   opinion  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
       "\n",
       "                                       stemmed_tweet  \\\n",
       "0                  ga hous hit um go chapel hill sat   \n",
       "1  theo walcott still shit uc watch rafa johnni d...   \n",
       "2      um gsp fan uc hate nick diaz ut wait februari   \n",
       "3  iranian gener say israel us iron dome ut deal ...   \n",
       "4  tehran uc mon amour obama tri establish tie mu...   \n",
       "\n",
       "                                   non_stemmed_tweet  \n",
       "0             gas house hit um going chapel hill sat  \n",
       "1  theo walcott still shit uc watch rafa johnny d...  \n",
       "2      um gsp fan uc hate nick diaz ut wait february  \n",
       "3  iranian general says israel us iron dome ut de...  \n",
       "4  tehran uc mon amour obama tried establish ties...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stemmed_tweet'] = stemmed_tweets\n",
    "data['non_stemmed_tweet'] = spaces_removed_tweets\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "accuracies = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Features extraction\n",
    "#### A. Trying Word embedding on the preprocessed Non stemmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-4a288e356599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#word embedding model build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/NU BDDS-PD/Practical Data Mining/word2vec/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#word embedding model build\n",
    "model = KeyedVectors.load_word2vec_format('E:/NU BDDS-PD/Practical Data Mining/word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the tweets to word embedding vectors using Google news W2V (300)\n",
    "vectorized_tweets = []\n",
    "for i in range(len(data.non_stemmed_tweet)):\n",
    "    #print(i)\n",
    "    tweets = []\n",
    "    words = data.non_stemmed_tweet[i].split()\n",
    "    length = len(words)\n",
    "    #print(type(words))\n",
    "    vector = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            #print(\"yes\")\n",
    "            vector += np.array(model[word])\n",
    "            #print(vector)\n",
    "        else:\n",
    "            vector += np.zeros(300)\n",
    "            #print(\"no\")\n",
    "            continue\n",
    "    #print(vector)\n",
    "    tweets = list(vector/length)\n",
    "    #print(type(tweets))\n",
    "    vectorized_tweets = np.append(vectorized_tweets, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tweets = np.reshape(vectorized_tweets, (-1,300), 'a')\n",
    "np.shape(w2v_tweets)\n",
    "#len(vectorized_tweets)/300\n",
    "\n",
    "w2v_tweets = pd.DataFrame(w2v_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting w2v_tweets into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the provided data as training data and the remaining 30% to test a classifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tweets_train,tweets_test,train_labels,test_labels = train_test_split(w2v_tweets,                   \n",
    "                                                 data['opinion'], test_size=0.3,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #will be used to scale the data between (0,1) to avoid -ve input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_tweets_train = scaler.fit_transform(tweets_train)\n",
    "scaled_tweets_test = scaler.fit_transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Non stemmed tweets into train & test and apply CountVectorizer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the provided data as training data and the remaining 30% to test a classifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train,tweets_test,train_labels,test_labels = train_test_split(data[\"non_stemmed_tweet\"],                   \n",
    "                                                 data['opinion'], test_size=0.3,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer:\n",
    "\n",
    "vectorizer = CountVectorizer().fit(tweets_train)\n",
    "\n",
    "# Training Dataset:\n",
    "tweets_train_vectorized = vectorizer.transform(tweets_train)\n",
    "\n",
    "# Test Dataset:\n",
    "tweets_test_vectorized = vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Training Dataset of Countvectorizer \"tweets_train_vectorized\" & Scaled Training Dataset of Word Embeddings \"scaled_tweets_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_vectorized_df = pd.DataFrame(tweets_train_vectorized.toarray()) # convert scipy.sparse.csr.csr_matrix to pandas df\n",
    "#tweets_train_vectorized_df.shape\n",
    "tweets_train_vectorized_df.iloc[0:6,25285:25290] # print the last 5 columns of \"tweets_train_vectorized_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_tweets_train_df = DataFrame(data=scaled_tweets_train, index=scaled_tweets_train.index)\n",
    "scaled_tweets_train_df = pd.DataFrame(data=scaled_tweets_train)\n",
    "scaled_tweets_train_df.head()\n",
    "scaled_tweets_train_df.iloc[0:6,295:300] # print the last 5 columns of \"scaled_tweets_train_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_train_vectorized_df.shape)\n",
    "print(scaled_tweets_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging tweets_train_vectorized_df & scaled_tweets_train_df:\n",
    "\n",
    "df_concat_train = pd.concat([tweets_train_vectorized_df, scaled_tweets_train_df], axis=1, ignore_index=True)\n",
    "df_concat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_concat_train.iloc[0:6,25285:25290])   # print the last 5 columns of \"tweets_train_vectorized_df\" after merging\n",
    "print(df_concat_train.iloc[0:6,25585:25590])   # print the last 5 columns of \"scaled_tweets_train_df\" after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_tweets_train_df.iloc[0:6,0:6]   # print the first 5 columns of \"scaled_tweets_train_df\" BEFORE merge\n",
    "df_concat_train.iloc[0:6,25285:25296]   # print the first 5 columns of \"scaled_tweets_train_df\" AFTER merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Test Dataset of Countvectorizer \"tweets_test_vectorized\" & Scaled Test Dataset of Word Embeddings \"scaled_tweets_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging tweets_test_vectorized_df & scaled_test_train_df:\n",
    "\n",
    "tweets_test_vectorized_df = pd.DataFrame(tweets_test_vectorized.toarray())\n",
    "scaled_tweets_test_df = pd.DataFrame(data=scaled_tweets_test)\n",
    "\n",
    "print(tweets_test_vectorized_df.shape)\n",
    "print(scaled_tweets_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_tweets_test_df.iloc[0:6,295:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_test = pd.concat([tweets_test_vectorized_df, scaled_tweets_test_df], axis=1, ignore_index=True)\n",
    "df_concat_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Merged data with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clfr=BaggingClassifier(base_estimator=LogisticRegression(C=0.3359818286283781), random_state=1,n_estimators=100)\n",
    "\n",
    "clfr.fit(df_concat_train,train_labels)\n",
    "\n",
    "predicted = clfr.predict(df_concat_test)\n",
    "acc = metrics.accuracy_score(test_labels,predicted)\n",
    "\n",
    "print ('Accuracy of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets) = '+str(acc*100)+'%')\n",
    "print (metrics.classification_report(test_labels,predicted))\n",
    "accuracies.append(('Accuracy of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets)', acc*100))\n",
    "f1_scores.append(('F1-score of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets)', metrics.f1_score(test_labels,predicted, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the provided Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_test = ['test.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('test.csv')\n",
    "#df3, df4 = [pd.read_csv(name, delimiter = '\\t', header = None) for name in files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Clean Up & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: casefold\n",
    "\n",
    "import nltk\n",
    "\n",
    "lowerTweets_test =[]\n",
    "for tweet in data_test['tweet']:\n",
    "    lowerTweets_test.append(tweet.casefold())\n",
    "lowerTweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: remove stopwords applying on all tweets \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "filtered_tweets_test =[]\n",
    "for doc in lowerTweets_test:\n",
    "    curr = \"\"\n",
    "    for word in  re.split(\"\\W+\",doc):\n",
    "        if word not in stops: \n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    filtered_tweets_test.append(curr)\n",
    "filtered_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove punctuation and digits from tweets and replace it by space\n",
    "#### NOTE: Through different combinations, it is observed that accuracy is decreased after removing punctuation.\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    output = []\n",
    "    for tweet in input_text:\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        output.append(tweet.translate(trantab))\n",
    "    return output\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    out_list = []\n",
    "    for j in input_text:\n",
    "        out_list.append(re.sub('\\d+', '', j))\n",
    "    return out_list\n",
    "\n",
    "#punctuation_removed_tweets = remove_punctuation(filtered_tweets)\n",
    "#punctuation_removed_tweets[0:5]\n",
    "\n",
    "# Here we will skip removing the punctuation,\n",
    "# and will use the \"remove_digits\" function with \"filtered_tweets\" (output of stopwords)\n",
    "digits_removed_tweets_test = remove_digits(filtered_tweets_test)\n",
    "digits_removed_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform trimming to remove extra whitespaces:\n",
    "\n",
    "spaces_removed_tweets_test = []\n",
    "for j in digits_removed_tweets_test:\n",
    "    spaces_removed_tweets_test.append(\" \".join(j.split()))\n",
    "\n",
    "spaces_removed_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['stemmed_tweet'] = stemmed_tweets\n",
    "data_test['preprocessed_tweet'] = spaces_removed_tweets_test\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply both CountVectorizer & Word Embeddings on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Applying CountVectorizer Provided Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CountVectorizer on Test Dataset:\n",
    "\n",
    "new_tweets_test_vectorized = vectorizer.transform(data_test['preprocessed_tweet'])\n",
    "new_tweets_test_vectorized\n",
    "#vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Applying W2V on Provided Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the w2v on the TEST TWEETS to word embedding vectors using Google news W2V (300):\n",
    "\n",
    "vectorized_test_tweets = []\n",
    "for i in range(len(data_test.preprocessed_tweet)):\n",
    "    #print(i)\n",
    "    tweets = []\n",
    "    words = data_test.preprocessed_tweet[i].split()\n",
    "    length = len(words)\n",
    "    #print(type(words))\n",
    "    vector = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            #print(\"yes\")\n",
    "            vector += np.array(model[word])\n",
    "            #print(vector)\n",
    "        else:\n",
    "            vector += np.zeros(300)\n",
    "            #print(\"no\")\n",
    "            continue\n",
    "    #print(vector)\n",
    "    tweets = list(vector/length)\n",
    "    #print(type(tweets))\n",
    "    vectorized_test_tweets = np.append(vectorized_test_tweets, tweets)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_tweets = np.reshape(vectorized_test_tweets, (-1,300), 'a')\n",
    "np.shape(w2v_test_tweets)\n",
    "#len(vectorized_tweets)/300\n",
    "\n",
    "w2v_test_tweets = pd.DataFrame(w2v_test_tweets)\n",
    "#w2v_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling w2v_test_tweets:\n",
    "scaler = MinMaxScaler()\n",
    "scaled_test_tweets = scaler.fit_transform(w2v_test_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Merging Vectorized Test Dataset of Countvectorizer \"new_tweets_test_vectorized\" & Scaled Test Dataset of Word Embeddings \"scaled_test_tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets_test_vectorized_df = pd.DataFrame(new_tweets_test_vectorized.toarray()) # convert scipy.sparse.csr.csr_matrix to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_tweets_df = pd.DataFrame(data=scaled_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging new_tweets_test_vectorized_df & scaled_test_tweets_df:\n",
    "\n",
    "df_merged_test = pd.concat([new_tweets_test_vectorized_df, scaled_test_tweets_df], axis=1, ignore_index=True)\n",
    "df_merged_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistic Regression on the Megred Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clfr = LogisticRegression(C=0.3359818286283781)\n",
    "clfr.fit(df_concat_train,train_labels)\n",
    "\n",
    "predicted_test = clfr.predict(df_merged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test\n",
    "print(len(predicted_test))\n",
    "print(predicted_test[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = []\n",
    "for prediction in range(len(predicted_test)):\n",
    "    if predicted_test[prediction] == 'neutral':\n",
    "        label_list.append(0)\n",
    "    if predicted_test[prediction] == 'positive':\n",
    "        label_list.append(1)\n",
    "    if predicted_test[prediction] == 'negative':\n",
    "        label_list.append(2)\n",
    "\n",
    "print(len(label_list))\n",
    "print(label_list[0:11])\n",
    "\n",
    "label_array = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_test[['id']].copy()\n",
    "result['label'] = pd.DataFrame(data=label_array)\n",
    "result.head(10)\n",
    "result.to_csv(\"CountVec_word2vec_bagging_smalldata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
